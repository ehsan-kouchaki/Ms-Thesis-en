{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9771f623-4f10-4aa9-8282-c675b21fb897",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mujoco_py\n",
    "from mujoco_py import load_model_from_path, MjSim, MjViewer\n",
    "import os\n",
    "import glfw\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import MultivariateNormal\n",
    "from torch.distributions import Beta\n",
    "from torch.optim import Adam\n",
    "from collections import deque\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import pickle\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c881d7bf-347e-4682-b88d-cb04f5c8a883",
   "metadata": {},
   "source": [
    "## network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e90a74c5-a328-4968-b5dc-70d0aeeb297b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNN(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, device, distribution):\n",
    "        super(PolicyNN, self).__init__()\n",
    "\n",
    "        self.layer1 = nn.Linear(in_dim, 512)\n",
    "        self.layer2 = nn.Linear(512, 256)\n",
    "        self.layer3 = nn.Linear(256, out_dim)\n",
    "        self.device = device\n",
    "        self.distribution = distribution\n",
    "\n",
    "    def forward(self, obs):\n",
    "        if isinstance(obs, np.ndarray):\n",
    "            obs = torch.tensor(obs, dtype=torch.float).to(self.device)\n",
    "\n",
    "        activation1 = F.relu(self.layer1(obs))\n",
    "        activation2 = F.relu(self.layer2(activation1))\n",
    "        out = self.layer3(activation2)\n",
    "        if self.distribution == 'Normal': output = out\n",
    "        elif self.distribution == 'Beta': output = F.softplus(out)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6e41c06-d68c-421c-ba3e-a8aeeefd5adb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PolicyNN(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, device, distribution):\n",
    "        super(PolicyNN, self).__init__()\n",
    "\n",
    "        self.layer1 = nn.Linear(in_dim, 100)\n",
    "        self.layer2 = nn.Linear(100, 50)\n",
    "        self.layer3 = nn.Linear(50, 25)\n",
    "        self.layer4 = nn.Linear(25, out_dim)\n",
    "        self.device = device\n",
    "        self.distribution = distribution\n",
    "\n",
    "    def forward(self, obs):\n",
    "        if isinstance(obs, np.ndarray):\n",
    "            obs = torch.tensor(obs, dtype=torch.float).to(self.device)\n",
    "\n",
    "        activation1 = F.relu(self.layer1(obs))\n",
    "        activation2 = F.relu(self.layer2(activation1))\n",
    "        activation3 = F.relu(self.layer3(activation2))\n",
    "        out = self.layer4(activation3)\n",
    "        if self.distribution == 'Normal': output = out\n",
    "        elif self.distribution == 'Beta': output = F.softplus(out)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53364f90-095c-438d-a4e4-beb20ed8edb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, ):\n",
    "        super()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee44d7d-9f6d-4c60-b766-eb98f2eaa389",
   "metadata": {},
   "source": [
    "## compute action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cd7692fe-5e38-4ba6-8f31-913fd0855474",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_action(actor, obs, cov_mat, distribution, action_range):\n",
    "    if distribution == 'Normal':        \n",
    "        mean = actor(obs)\n",
    "        dist = MultivariateNormal(mean, cov_mat)\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action)\n",
    "    elif distribution == 'Beta':\n",
    "        alpha_beta = actor(obs)\n",
    "        alpha = alpha_beta[0: int(alpha_beta.shape[0]/2)] + 1\n",
    "        beta = alpha_beta[int(alpha_beta.shape[0]/2): int(alpha_beta.shape[0])] + 1\n",
    "        dist = Beta(alpha, beta)\n",
    "        action0_1 = dist.sample()\n",
    "        # to map [0, 1] to [act_minrange, act_maxrange]:\n",
    "        action = (torch.tensor(action_range[1]) - torch.tensor(action_range[0])\n",
    "                 ) * action0_1 + torch.tensor(action_range[0])\n",
    "        log_prob = dist.log_prob(action0_1).mean()\n",
    "    return action.detach().cpu().numpy(), log_prob    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc8a8d9-5a3f-40f8-b79b-5387f611355f",
   "metadata": {},
   "source": [
    "## compute Q(s|a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d44783eb-693d-4f12-9e30-e68a9f06ca4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rtgs(batch_rews, gamma, device):\n",
    "    batch_rtgs = []\n",
    "\n",
    "    for ep_rews in reversed(batch_rews):\n",
    "        discounted_reward = 0 \n",
    "\n",
    "        for rew in reversed(ep_rews):\n",
    "            discounted_reward = rew + discounted_reward * gamma\n",
    "            batch_rtgs.insert(0, discounted_reward)\n",
    "\n",
    "    batch_rtgs = torch.tensor(batch_rtgs, dtype=torch.float).to(device)\n",
    "    return batch_rtgs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e271fefc-f79f-4a51-871c-3634f8f66524",
   "metadata": {
    "tags": []
   },
   "source": [
    "## evaluate V(S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "143a6ccd-e81b-437d-9d0d-c6e16babe026",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(actor, critic, cov_mat, batch_obs, batch_acts, distribution, action_range):\n",
    "    V = critic(batch_obs).squeeze()\n",
    "    # This segment of code is similar to that in get_action()\n",
    "    if distribution == 'Normal':\n",
    "        mean = actor(batch_obs)\n",
    "        dist = MultivariateNormal(mean, cov_mat)\n",
    "        log_probs = dist.log_prob(batch_acts)\n",
    "    elif distribution == 'Beta':\n",
    "        alpha_beta = actor(batch_obs)\n",
    "        alpha = alpha_beta[:, 0: int(alpha_beta.shape[1]/2)] + 1\n",
    "        beta = alpha_beta[:, int(alpha_beta.shape[1]/2): int(alpha_beta.shape[1])] + 1\n",
    "        dist = Beta(alpha , beta)\n",
    "        #to remapmap [act_minrange, act_maxrange] to [0, 1]\n",
    "        max_min = torch.tensor(action_range[1]) - torch.tensor(action_range[0])\n",
    "        minn = torch.tensor(action_range[0])                     \n",
    "        batch_acts0_1 = (batch_acts - minn)*(1-2e-5)/max_min + 1e-5 # to avoind beyound [0,1]\n",
    "        log_probs = dist.log_prob(batch_acts0_1).mean(dim=1)\n",
    "        \n",
    "    return V, log_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "039fc8a7-7eac-4dc0-92f6-b3aa71623cf0",
   "metadata": {},
   "source": [
    "## get reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63cc37a8-f131-41b3-bc78-3ccae8e61ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def euler_from_quaternion(quaternion):\n",
    "    w = quaternion[0]; x = quaternion[1]; y = quaternion[2]; z = quaternion[3]\n",
    "    t0 = +2.0 * (w * x + y * z)\n",
    "    t1 = +1.0 - 2.0 * (x * x + y * y)\n",
    "    roll_x = np.arctan2(t0, t1)\n",
    "\n",
    "    t2 = +2.0 * (w * y - z * x)\n",
    "    t2 = +1.0 if t2 > +1.0 else t2\n",
    "    t2 = -1.0 if t2 < -1.0 else t2\n",
    "    pitch_y = np.arcsin(t2)\n",
    "\n",
    "    t3 = +2.0 * (w * z + x * y)\n",
    "    t4 = +1.0 - 2.0 * (y * y + z * z)\n",
    "    yaw_z = np.arctan2(t3, t4)\n",
    "\n",
    "    return roll_x, pitch_y, yaw_z \n",
    "\n",
    "def compute_reward(e_i, alpha_i):\n",
    "    return np.exp(-alpha_i * e_i**2)\n",
    "\n",
    "def get_reward(alpha, COM, COM_vel, quaternion, support_center, fall,\n",
    "               w1 = 1.0/8,    w2 = 1.5/8,     w3 = 1/8,\n",
    "               w4 = 0.5/8,    w5 = 0.5/8,    w6 = 0.5/8,\n",
    "               w7 = 1.5/8,    w8 = 1.5/8):\n",
    "\n",
    "    e_i = abs(support_center[0]-0.015 - COM[0])\n",
    "    r_xCOM = compute_reward(e_i, alpha[0])\n",
    "    \n",
    "    e_i = abs(support_center[1] - COM[1])\n",
    "    r_yCOM = compute_reward(e_i, alpha[1])\n",
    "    \n",
    "    e_i = abs(0.47 - COM[2])\n",
    "    r_zCOM = compute_reward(e_i, alpha[2])\n",
    "\n",
    "    e_i = COM_vel[0];    r_xdotCOM = compute_reward(e_i, alpha[3])\n",
    "    e_i = COM_vel[1];    r_ydotCOM = compute_reward(e_i, alpha[3])\n",
    "    e_i = COM_vel[2];    r_zdotCOM = compute_reward(e_i, alpha[3])\n",
    "    \n",
    "    roll_x, pitch_y, yaw_z = euler_from_quaternion(quaternion)\n",
    "    e_i = roll_x;     r_roll  = compute_reward(e_i, alpha[4])\n",
    "    e_i = pitch_y;    r_pitch = compute_reward(e_i, alpha[4])\n",
    "    \n",
    "    if not fall:\n",
    "        total_reward = (w1 * r_xCOM + w2 * r_yCOM + w3 * r_zCOM +\n",
    "                        w4 * r_xdotCOM + w5 * r_ydotCOM + w6 * r_zdotCOM +\n",
    "                        w7 * r_roll + w8 * r_pitch)\n",
    "    else:\n",
    "        total_reward = 0\n",
    "    return  total_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d20b556d-d20e-489f-82b4-5e8203081952",
   "metadata": {},
   "source": [
    "## initial condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "14bfbf23-82b4-4f37-b179-7a325ca7d694",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_condition(sim, model, kp, kd, ki, mode, ball):    \n",
    "    if mode == 'test': viewer = MjViewer(sim)\n",
    "    if not ball:\n",
    "        hip_torque = 1.0*np.random.uniform(low=1.0, high=4.2)\n",
    "        knee_torque = 1.0*np.random.uniform(low=1.4, high=4.6)\n",
    "        ankle_torque = 1.0*np.random.uniform(low=1.0, high=4.2)\n",
    "        sim_time = np.random.uniform(low=60, high=120)\n",
    "        for _ in range(int(sim_time)):\n",
    "            sim.data.ctrl[:] = [0, hip_torque, knee_torque, ankle_torque, 0,\n",
    "                                0, hip_torque, knee_torque, ankle_torque, 0]\n",
    "            sim.step()\n",
    "            if mode == 'test':\n",
    "                viewer.render()\n",
    "                time.sleep(0.01)\n",
    "\n",
    "    elif ball:\n",
    "        sim_time = np.random.uniform(low=140, high=180)\n",
    "        shoot_impulse = np.random.uniform(low=2.5, high=3)\n",
    "        shoot_angle = np.random.uniform(low=30, high=45)*np.pi/180\n",
    "        shoot_duration = sim_time * model.opt.timestep\n",
    "        shoot_force = shoot_impulse/shoot_duration\n",
    "        external_push_x = -shoot_force * np.cos(shoot_angle)\n",
    "        external_push_z =  shoot_force * np.sin(shoot_angle)\n",
    "        qpos_d = np.array([0]*10)\n",
    "        qvel_d = np.array([0]*10)\n",
    "        int_pos = np.zeros(10)\n",
    "        old_joints_position = [0]*10\n",
    "        \n",
    "        sim.data.xfrc_applied[24][:3] = [external_push_x, 0, external_push_z]\n",
    "        sim_state = sim.get_state()\n",
    "        joints_position = sim_state.qpos[7:17]\n",
    "        joints_velocity = sim_state.qvel[6:16]\n",
    "        for i in range(int(sim_time)):\n",
    "            int_pos += model.opt.timestep * (( np.array(joints_position)\n",
    "                                              +np.array(old_joints_position)) / 2-qpos_d)\n",
    "            error = qpos_d - joints_position\n",
    "            derror = qvel_d - joints_velocity\n",
    "            apllied_torques = kp * error + kd * derror -ki *int_pos\n",
    "            sim.data.ctrl[:] = apllied_torques\n",
    "            #if i > sim_time - 2: sim.data.ctrl[:] = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "            sim.step()\n",
    "            if mode == 'test':\n",
    "                viewer.render()\n",
    "                time.sleep(0.01)\n",
    "            old_joints_position = joints_position\n",
    "\n",
    "            sim_state = sim.get_state()\n",
    "            joints_position = sim_state.qpos[7:17]\n",
    "            joints_velocity = sim_state.qvel[6:16]\n",
    "    \n",
    "    sim.data.time = 0\n",
    "    sim.data.ctrl[:] = 0\n",
    "    sim.data.xfrc_applied[:] = 0\n",
    "    '''sim_state = sim.get_state()\n",
    "    sim_state.qvel[:] = np.zeros(18)\n",
    "    sim.set_state(sim_state)'''\n",
    "    if mode == 'test': glfw.destroy_window(viewer.window)\n",
    "    return sim\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03af080c-d54b-48b3-b517-88dd1e68adaf",
   "metadata": {},
   "source": [
    "## Contact forces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c5130fe-accd-4d08-a021-10b2f2876978",
   "metadata": {},
   "outputs": [],
   "source": [
    "def contact_force(sim):\n",
    "    #contact_forces = np.zeros(8, dtype=np.float64)\n",
    "    #contact_forces = np.zeros(2, dtype=np.float64)\n",
    "    left_contact = []\n",
    "    right_contact = []\n",
    "    for i in range(sim.data.ncon):\n",
    "        c_array = np.zeros(6, dtype=np.float64)\n",
    "        contact = sim.data.contact[i]\n",
    "        if contact.geom1 == 0 and contact.geom2 == 10:\n",
    "            mujoco_py.functions.mj_contactForce(sim.model, sim.data, i, c_array)\n",
    "            left_contact.append(c_array[0])\n",
    "        elif contact.geom1 == 0 and contact.geom2 == 19:\n",
    "            mujoco_py.functions.mj_contactForce(sim.model, sim.data, i, c_array)\n",
    "            right_contact.append(c_array[0])\n",
    "    \n",
    "    '''if len(left_contact) == 4:\n",
    "        contact_forces[0:4] = left_contact\n",
    "    else:\n",
    "        left_sensor = np.array([sim.data.sensordata[8], sim.data.sensordata[11],\n",
    "                                sim.data.sensordata[14] ,sim.data.sensordata[17]])\n",
    "        l_s_sort_indx = left_sensor.argsort()\n",
    "        l_contact_sort = sorted(left_contact,reverse=True)\n",
    "        for i in range(len(left_contact)):\n",
    "            contact_forces[l_s_sort_indx[i]] = l_contact_sort[i]\n",
    "    \n",
    "    if len(right_contact) == 4:\n",
    "        contact_forces[4:8] = right_contact\n",
    "    else:\n",
    "        right_sensor = np.array([sim.data.sensordata[20], sim.data.sensordata[23],\n",
    "                                 sim.data.sensordata[26], sim.data.sensordata[29]])\n",
    "        r_s_sort_indx = right_sensor.argsort()\n",
    "        r_contact_sort = sorted(right_contact,reverse=True)\n",
    "        for i in range(len(right_contact)):\n",
    "            contact_forces[r_s_sort_indx[i] + 4] = r_contact_sort[i]\n",
    "    return contact_forces'''\n",
    "    #contact_forces[0] = sum(left_contact)\n",
    "    #contact_forces[1] = sum(right_contact)\n",
    "    return left_contact, right_contact"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb09e9d-e0ef-4c41-ba94-266d62c6d91c",
   "metadata": {},
   "source": [
    "## PID step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2a890e9-07fc-4e45-81d1-9305e74e6ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pid_step(sim, model, pid_time, kp, kd, ki, qpos_d, state_range, mode, \n",
    "             qvel_d = np.array([0]*10), int_pos = np.zeros(10), old_joints_position = [0]*10,\n",
    "             old_left_contact = [], old_right_contact =[]):\n",
    "    if mode == 'test': viewer = MjViewer(sim)\n",
    "    sim_state = sim.get_state()\n",
    "    joints_position = sim_state.qpos[7:17]\n",
    "    joints_velocity = sim_state.qvel[6:16]\n",
    "    fall = False\n",
    "    for step in range(int(pid_time / model.opt.timestep)):\n",
    "        int_pos += model.opt.timestep * (( np.array(joints_position)\n",
    "                                          +np.array(old_joints_position)) / 2-qpos_d)\n",
    "        error = qpos_d - joints_position\n",
    "        derror = qvel_d - joints_velocity\n",
    "        applied_torques = kp * error + kd * derror -ki *int_pos\n",
    "        sim.data.ctrl[:] = applied_torques\n",
    "        \n",
    "        sim.step()\n",
    "        if mode == 'test':\n",
    "            viewer.render()\n",
    "            time.sleep(0.002)\n",
    "        old_joints_position = joints_position\n",
    "        \n",
    "        sim_state = sim.get_state()\n",
    "        joints_position = sim_state.qpos[7:17]\n",
    "        joints_velocity = sim_state.qvel[6:16]\n",
    "        \n",
    "        contact0 = sim.data.contact[0]\n",
    "        geom_1 = sim.model.geom_id2name(contact0.geom1)\n",
    "        geom_2 = sim.model.geom_id2name(contact0.geom2)\n",
    "        '''\n",
    "        if (sim.data.ncon > 0 and geom_1 == 'floor' and \\\n",
    "            geom_2 not in ('l_foot_link', 'r_foot_link', 'small_ball',\n",
    "                           'l_ank_roll_link', 'r_ank_roll_link')) or\\\n",
    "        sim.data.sensordata[2]> 0.6 :\n",
    "            fall = True\n",
    "            break\n",
    "        '''\n",
    "        left_contact, right_contact = contact_force(sim)\n",
    "        if sim.data.ncon > 0 and ((step>35 and \n",
    "                                  (len(left_contact)<2 or len(right_contact)<2)) or\n",
    "                                  (geom_1 == 'floor' and geom_2 not in ('l_foot_link', 'r_foot_link',\n",
    "                                                                        'small_ball',  'l_ank_roll_link', 'r_ank_roll_link')) or\n",
    "                                  sim.data.sensordata[2]> 0.6):\n",
    "            fall = True\n",
    "            break\n",
    "        old_left_contact = left_contact\n",
    "        old_right_contact = right_contact\n",
    "    if mode == 'test': glfw.destroy_window(viewer.window)\n",
    "    sup_cent = [0.5 * (sim.data.sensordata[30] + sim.data.sensordata[33]),\n",
    "                0.5 * (sim.data.sensordata[31] + sim.data.sensordata[34])]\n",
    "    COM_xyz = sim.data.sensordata[:3]\n",
    "    COM_vel = sim.data.sensordata[3:6]\n",
    "    sum_contact_forces = np.array([sum(old_left_contact), \n",
    "                                   sum(old_right_contact)])\n",
    "    new_obs = np.concatenate((sim_state.qpos[0:17], sim_state.qvel[0:16],\n",
    "                              sum_contact_forces))\n",
    "    '''new_obs = np.concatenate((sim_state.qpos[7:17], sim_state.qvel[6:16],\n",
    "                             [sim_state.qpos[3]]))'''\n",
    "    quaternion = [sim.data.qpos[3], sim.data.qpos[4],\n",
    "                  sim.data.qpos[5], sim.data.qpos[6]]\n",
    "    return new_obs, COM_xyz, COM_vel, quaternion, sup_cent, fall"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b392e0f3-300c-40f1-acc9-f4253d289513",
   "metadata": {},
   "source": [
    "## rollout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6b00725f-b9cf-409f-88d7-e285018d100c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rollout(model, pid_time, kp, kd, ki, actor, cov_mat, gamma, rew_coef, distribution,\n",
    "            timesteps_per_batch, max_timesteps_per_episode, action_range,\n",
    "            device, mode, ball, obs_dim):\n",
    "    batch_obs = []\n",
    "    batch_acts = []\n",
    "    batch_log_probs = []\n",
    "    batch_rews = []\n",
    "    batch_rtgs = []\n",
    "    batch_lens = []\n",
    "    sum_ep_rew = []\n",
    "    t = 0\n",
    "    while t < timesteps_per_batch:\n",
    "        ep_rews = [] # rewards collected per episode\n",
    "\n",
    "        sim = MjSim(model) \n",
    "        sim = initial_condition(sim, model, kp, kd, ki, mode, ball)\n",
    "        left_contact, right_contact = contact_force(sim)\n",
    "        sum_contact_forces = np.array([sum(left_contact), sum(right_contact)])\n",
    "        obs = np.concatenate((sim.data.qpos[0:17], sim.data.qvel[0:16], sum_contact_forces))\n",
    "    \n",
    "        done = False\n",
    "        ep_length = 0\n",
    "        for ep_t in range(max_timesteps_per_episode):\n",
    "            t += 1 \n",
    "            ep_length += 1\n",
    "\n",
    "            batch_obs.append(obs)\n",
    "            action, log_prob = get_action(actor, obs, cov_mat, distribution, action_range)\n",
    "            #q_des = np.concatenate((action, action))\n",
    "            q_des = action + obs[7:17]\n",
    "            obs, COM_xyz, COM_vel, quat, sup_cent, fall = \\\n",
    "            pid_step(sim, model, pid_time, kp, kd, ki, q_des, state_range, mode)\n",
    "            if fall or ep_length == max_timesteps_per_episode: done = True \n",
    "            rew = get_reward(rew_coef, COM_xyz, COM_vel, quat, sup_cent, fall) * (1 - done)\n",
    "\n",
    "            ep_rews.append(rew)\n",
    "            batch_acts.append(action)\n",
    "            batch_log_probs.append(log_prob)\n",
    "\n",
    "            if done: break\n",
    "\n",
    "        sum_ep_rew.append(np.sum(ep_rews))\n",
    "        batch_lens.append(ep_t + 1)\n",
    "        batch_rews.append(ep_rews)\n",
    "    batch_obs = torch.tensor(np.array(batch_obs), dtype=torch.float).to(device)\n",
    "    batch_acts = torch.tensor(np.array(batch_acts), dtype=torch.float).to(device)\n",
    "    batch_log_probs = torch.tensor(batch_log_probs, dtype=torch.float).to(device)\n",
    "    batch_rtgs = compute_rtgs(batch_rews, gamma, device)                                                              # ALG STEP 4\n",
    "    return batch_obs, batch_acts, batch_log_probs, batch_rtgs, batch_lens, batch_rews, sum_ep_rew\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87418f64-5444-4c77-a339-251eec60ccde",
   "metadata": {},
   "source": [
    "## model and parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ca182da2-9cfc-4f90-a608-e994db4f12f9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Specify the distribution type. Available choices are Normal and Bete \n",
      " Beta\n"
     ]
    }
   ],
   "source": [
    "distribution = input('Specify the distribution type. Available choices are Normal and Bete \\n')\n",
    "while distribution not in ('Normal', 'Beta'):\n",
    "    print('Undefined', distribution, 'distribution \\n')\n",
    "    distribution = input('Specify the distribution type. Available choices are Normal and Bete \\n')\n",
    "mj_path = mujoco_py.utils.discover_mujoco()\n",
    "ball = False\n",
    "xml_path = os.path.join(mj_path, 'model', 'aiutman_lower_rollpitch.xml')\n",
    "\n",
    "model = load_model_from_path(xml_path)\n",
    "sim = MjSim(model)\n",
    "pid_time = 0.08\n",
    "\n",
    "control_set = {#              [Kp,    Kd,     ki,    ac_min, ac_max, pos_min, pos_max]\n",
    "               \"l_hip_roll\":  [40,    0.1,   15,    -0.05,   0.05,   -0.50,    0.50], #0\n",
    "               \"l_hip_pitch\": [80,    0.1,   15,    -0.05,   0.05,   -1.57,    1.57], #1\n",
    "               \"l_knee\":      [80,    0.1,   15,    -0.05,   0.05,    0.00,    2.80], #2\n",
    "               \"l_ank_pitch\": [80,    0.1,   15,    -0.05,   0.05,   -1.40,    1.40], #3\n",
    "               \"l_ank_roll\":  [80,    0.1,   15,    -0.05,   0.05,   -0.50,    0.50], #4\n",
    "               \"r_hip_roll\":  [40,    0.1,   15,    -0.05,   0.05,   -0.50,    0.50], #5\n",
    "               \"r_hip_pitch\": [80,    0.1,   15,    -0.05,   0.05,   -1.57,    1.57], #6\n",
    "               \"r_knee\":      [80,    0.1,   15,    -0.05,   0.05,    0.00,    2.80], #7\n",
    "               \"r_ank_pitch\": [80,    0.1,   15,    -0.05,   0.05,   -1.40,    1.40], #8\n",
    "               \"r_ank_roll\":  [80,    0.1,   15,    -0.05,   0.05,   -0.50,    0.50], #9\n",
    "               }\n",
    "\n",
    "kp = np.array(list(control_set.values()))[: , 0]\n",
    "kd = np.array(list(control_set.values()))[: , 1]\n",
    "ki = np.array(list(control_set.values()))[: , 2]\n",
    "act_minrange = np.array(list(control_set.values()))[: , 3]\n",
    "act_maxrange = np.array(list(control_set.values()))[: , 4]\n",
    "pos_minrange = np.array(list(control_set.values()))[: , 5]\n",
    "pos_maxrange = np.array(list(control_set.values()))[: , 6]\n",
    "action_range = [list(act_minrange), list(act_maxrange)]\n",
    "state_range = [list(pos_minrange), list(pos_maxrange)]\n",
    "\n",
    "mass = 5.2       # total mass exept feet\n",
    "mass_f = 0.4        # feet mass\n",
    "lx = 0.208            # foot length\n",
    "ly = 0.132            # foot width\n",
    "r_pendulum = 0.43    # pendulum length\n",
    "g = 9.81              # gravity constant\n",
    "\n",
    "epsilon = 1e-5\n",
    "\n",
    "e_xCOM_max = lx / 2;\n",
    "e_yCOM_max = ly;\n",
    "e_zCOM_max = 0.3;\n",
    "\n",
    "thetadot_crt = np.sqrt(g*(mass + mass_f)/(mass * r_pendulum))\n",
    "e_vCOM_max = r_pendulum * thetadot_crt\n",
    "e_quat_max = np.pi/6;\n",
    "\n",
    "alpha_xCOM = -np.log(epsilon)/(e_xCOM_max ** 2)\n",
    "alpha_yCOM = -np.log(epsilon)/(e_yCOM_max ** 2)\n",
    "alpha_zCOM = -np.log(epsilon)/(e_zCOM_max ** 2)\n",
    "alpha_vCOM = -np.log(epsilon)/(e_vCOM_max ** 2)\n",
    "alpha_quat = -np.log(epsilon)/(e_quat_max ** 2)\n",
    "rew_coef = [alpha_xCOM, alpha_yCOM, alpha_zCOM, alpha_vCOM, alpha_quat]\n",
    "\n",
    "state_dim = 2*(len(model.joint_names) - 1) - 2\n",
    "obs_dim = state_dim + 15\n",
    "if distribution == 'Normal': act_dim = int(state_dim / 2)\n",
    "elif distribution == 'Beta': act_dim = 2*int(state_dim / 2)\n",
    "\n",
    "actor = PolicyNN(obs_dim, act_dim, device, distribution).to(device)\n",
    "critic = PolicyNN(obs_dim, 1, device, distribution).to(device)\n",
    "\n",
    "learning_rate = 0.00005\n",
    "actor_optim = Adam(actor.parameters(), lr=learning_rate)\n",
    "critic_optim = Adam(critic.parameters(), lr=learning_rate)\n",
    "\n",
    "cov_var = torch.full(size=(act_dim,), fill_value=0.01)\n",
    "cov_mat = torch.diag(cov_var).to(device)\n",
    "\n",
    "max_timesteps_per_episode = 100\n",
    "timesteps_per_batch = 10 * max_timesteps_per_episode\n",
    "total_timesteps = 100000000\n",
    "Epochs = 5\n",
    "clip = 0.2\n",
    "gamma = 0.99\n",
    "entropy_beta = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870d93c4-0541-434b-a63a-6ab3aca28fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "actor.load_state_dict(torch.load('act_rollpitch_2layer256_epoch5_lr50_baseobsAndSUMcontact'))\n",
    "critic.load_state_dict(torch.load('crt_rollpitch_2layer256_epoch5_lr50_baseobsAndSUMcontact'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4371c9de-20a0-436a-8aa2-d030015e72b8",
   "metadata": {},
   "source": [
    "## train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6553c81a-74ae-4eea-bf6e-8afe27c00281",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In iteratin 1 mean ep length is: 2.48 and mean ep rewards is: 1.20\n",
      "maximum mean reward is acheived!\n",
      "In iteratin 2 mean ep length is: 2.27 and mean ep rewards is: 1.06\n",
      "In iteratin 3 mean ep length is: 2.63 and mean ep rewards is: 1.32\n",
      "maximum mean reward is acheived!\n",
      "In iteratin 4 mean ep length is: 2.64 and mean ep rewards is: 1.33\n",
      "maximum mean reward is acheived!\n",
      "In iteratin 5 mean ep length is: 2.71 and mean ep rewards is: 1.34\n",
      "maximum mean reward is acheived!\n",
      "In iteratin 6 mean ep length is: 2.69 and mean ep rewards is: 1.33\n",
      "In iteratin 7 mean ep length is: 2.75 and mean ep rewards is: 1.37\n",
      "maximum mean reward is acheived!\n",
      "In iteratin 8 mean ep length is: 2.81 and mean ep rewards is: 1.43\n",
      "maximum mean reward is acheived!\n",
      "In iteratin 9 mean ep length is: 3.03 and mean ep rewards is: 1.59\n",
      "maximum mean reward is acheived!\n",
      "In iteratin 10 mean ep length is: 3.59 and mean ep rewards is: 2.00\n",
      "maximum mean reward is acheived!\n",
      "In iteratin 11 mean ep length is: 3.02 and mean ep rewards is: 1.56\n",
      "In iteratin 12 mean ep length is: 3.35 and mean ep rewards is: 1.81\n",
      "In iteratin 13 mean ep length is: 3.41 and mean ep rewards is: 1.81\n",
      "In iteratin 14 mean ep length is: 3.46 and mean ep rewards is: 1.85\n",
      "In iteratin 15 mean ep length is: 3.32 and mean ep rewards is: 1.77\n",
      "In iteratin 16 mean ep length is: 3.06 and mean ep rewards is: 1.52\n",
      "In iteratin 17 mean ep length is: 3.35 and mean ep rewards is: 1.74\n",
      "In iteratin 18 mean ep length is: 2.88 and mean ep rewards is: 1.43\n",
      "In iteratin 19 mean ep length is: 3.28 and mean ep rewards is: 1.70\n",
      "In iteratin 20 mean ep length is: 2.65 and mean ep rewards is: 1.30\n",
      "In iteratin 21 mean ep length is: 2.50 and mean ep rewards is: 1.18\n",
      "In iteratin 22 mean ep length is: 2.16 and mean ep rewards is: 0.90\n",
      "In iteratin 23 mean ep length is: 2.08 and mean ep rewards is: 0.88\n",
      "In iteratin 24 mean ep length is: 1.85 and mean ep rewards is: 0.69\n",
      "In iteratin 25 mean ep length is: 1.97 and mean ep rewards is: 0.80\n",
      "In iteratin 26 mean ep length is: 1.98 and mean ep rewards is: 0.80\n",
      "In iteratin 27 mean ep length is: 2.36 and mean ep rewards is: 1.11\n",
      "In iteratin 28 mean ep length is: 2.94 and mean ep rewards is: 1.52\n",
      "In iteratin 29 mean ep length is: 2.83 and mean ep rewards is: 1.40\n",
      "In iteratin 30 mean ep length is: 3.47 and mean ep rewards is: 1.87\n",
      "In iteratin 31 mean ep length is: 3.47 and mean ep rewards is: 1.83\n",
      "In iteratin 32 mean ep length is: 3.54 and mean ep rewards is: 1.90\n",
      "In iteratin 33 mean ep length is: 3.74 and mean ep rewards is: 2.08\n",
      "maximum mean reward is acheived!\n",
      "In iteratin 34 mean ep length is: 3.72 and mean ep rewards is: 2.02\n",
      "In iteratin 35 mean ep length is: 3.46 and mean ep rewards is: 1.86\n",
      "In iteratin 36 mean ep length is: 3.89 and mean ep rewards is: 2.14\n",
      "maximum mean reward is acheived!\n",
      "In iteratin 37 mean ep length is: 3.58 and mean ep rewards is: 1.92\n",
      "In iteratin 38 mean ep length is: 3.58 and mean ep rewards is: 1.97\n",
      "In iteratin 39 mean ep length is: 3.60 and mean ep rewards is: 1.96\n",
      "In iteratin 40 mean ep length is: 3.22 and mean ep rewards is: 1.65\n",
      "In iteratin 41 mean ep length is: 3.01 and mean ep rewards is: 1.52\n",
      "In iteratin 42 mean ep length is: 2.97 and mean ep rewards is: 1.45\n",
      "In iteratin 43 mean ep length is: 3.04 and mean ep rewards is: 1.52\n",
      "In iteratin 44 mean ep length is: 2.64 and mean ep rewards is: 1.22\n",
      "In iteratin 45 mean ep length is: 2.49 and mean ep rewards is: 1.15\n",
      "In iteratin 46 mean ep length is: 2.83 and mean ep rewards is: 1.34\n",
      "In iteratin 47 mean ep length is: 3.03 and mean ep rewards is: 1.48\n",
      "In iteratin 48 mean ep length is: 3.07 and mean ep rewards is: 1.51\n",
      "In iteratin 49 mean ep length is: 2.96 and mean ep rewards is: 1.40\n",
      "In iteratin 50 mean ep length is: 3.17 and mean ep rewards is: 1.62\n",
      "In iteratin 51 mean ep length is: 3.07 and mean ep rewards is: 1.52\n",
      "In iteratin 52 mean ep length is: 3.25 and mean ep rewards is: 1.63\n",
      "In iteratin 53 mean ep length is: 2.98 and mean ep rewards is: 1.44\n",
      "In iteratin 54 mean ep length is: 2.97 and mean ep rewards is: 1.44\n",
      "In iteratin 55 mean ep length is: 2.84 and mean ep rewards is: 1.35\n",
      "In iteratin 56 mean ep length is: 2.66 and mean ep rewards is: 1.24\n",
      "In iteratin 57 mean ep length is: 2.75 and mean ep rewards is: 1.28\n",
      "In iteratin 58 mean ep length is: 2.71 and mean ep rewards is: 1.23\n",
      "In iteratin 59 mean ep length is: 2.53 and mean ep rewards is: 1.15\n",
      "In iteratin 60 mean ep length is: 2.57 and mean ep rewards is: 1.23\n",
      "In iteratin 61 mean ep length is: 2.18 and mean ep rewards is: 0.94\n",
      "In iteratin 62 mean ep length is: 2.23 and mean ep rewards is: 0.96\n",
      "In iteratin 63 mean ep length is: 2.67 and mean ep rewards is: 1.29\n",
      "In iteratin 64 mean ep length is: 3.50 and mean ep rewards is: 1.85\n",
      "In iteratin 65 mean ep length is: 3.49 and mean ep rewards is: 1.81\n",
      "In iteratin 66 mean ep length is: 3.53 and mean ep rewards is: 1.80\n",
      "In iteratin 67 mean ep length is: 3.03 and mean ep rewards is: 1.47\n",
      "In iteratin 68 mean ep length is: 2.88 and mean ep rewards is: 1.38\n",
      "In iteratin 69 mean ep length is: 2.19 and mean ep rewards is: 0.88\n",
      "In iteratin 70 mean ep length is: 2.21 and mean ep rewards is: 0.92\n",
      "In iteratin 71 mean ep length is: 2.34 and mean ep rewards is: 1.00\n",
      "In iteratin 72 mean ep length is: 2.11 and mean ep rewards is: 0.82\n",
      "In iteratin 73 mean ep length is: 2.50 and mean ep rewards is: 1.10\n",
      "In iteratin 74 mean ep length is: 2.82 and mean ep rewards is: 1.29\n",
      "In iteratin 75 mean ep length is: 3.34 and mean ep rewards is: 1.64\n",
      "In iteratin 76 mean ep length is: 3.22 and mean ep rewards is: 1.56\n",
      "In iteratin 77 mean ep length is: 3.32 and mean ep rewards is: 1.61\n",
      "In iteratin 78 mean ep length is: 3.38 and mean ep rewards is: 1.68\n",
      "In iteratin 79 mean ep length is: 3.93 and mean ep rewards is: 2.10\n",
      "In iteratin 80 mean ep length is: 3.20 and mean ep rewards is: 1.55\n",
      "In iteratin 81 mean ep length is: 3.37 and mean ep rewards is: 1.68\n",
      "In iteratin 82 mean ep length is: 3.46 and mean ep rewards is: 1.72\n",
      "In iteratin 83 mean ep length is: 3.27 and mean ep rewards is: 1.63\n",
      "In iteratin 84 mean ep length is: 3.33 and mean ep rewards is: 1.65\n",
      "In iteratin 85 mean ep length is: 3.26 and mean ep rewards is: 1.64\n",
      "In iteratin 86 mean ep length is: 2.63 and mean ep rewards is: 1.22\n",
      "In iteratin 87 mean ep length is: 2.08 and mean ep rewards is: 0.85\n",
      "In iteratin 88 mean ep length is: 1.84 and mean ep rewards is: 0.69\n",
      "In iteratin 89 mean ep length is: 1.82 and mean ep rewards is: 0.67\n",
      "In iteratin 90 mean ep length is: 1.82 and mean ep rewards is: 0.66\n",
      "In iteratin 91 mean ep length is: 1.88 and mean ep rewards is: 0.72\n",
      "In iteratin 92 mean ep length is: 2.51 and mean ep rewards is: 1.20\n",
      "In iteratin 93 mean ep length is: 2.87 and mean ep rewards is: 1.45\n",
      "In iteratin 94 mean ep length is: 2.91 and mean ep rewards is: 1.44\n",
      "In iteratin 95 mean ep length is: 3.24 and mean ep rewards is: 1.62\n",
      "In iteratin 96 mean ep length is: 3.95 and mean ep rewards is: 2.05\n",
      "In iteratin 97 mean ep length is: 3.70 and mean ep rewards is: 1.88\n",
      "In iteratin 98 mean ep length is: 3.32 and mean ep rewards is: 1.65\n",
      "In iteratin 99 mean ep length is: 3.26 and mean ep rewards is: 1.65\n",
      "In iteratin 100 mean ep length is: 2.54 and mean ep rewards is: 1.13\n",
      "In iteratin 101 mean ep length is: 2.54 and mean ep rewards is: 1.13\n",
      "In iteratin 102 mean ep length is: 2.38 and mean ep rewards is: 1.04\n",
      "In iteratin 103 mean ep length is: 2.24 and mean ep rewards is: 0.92\n",
      "In iteratin 104 mean ep length is: 2.41 and mean ep rewards is: 1.03\n",
      "In iteratin 105 mean ep length is: 2.22 and mean ep rewards is: 0.91\n",
      "In iteratin 106 mean ep length is: 2.06 and mean ep rewards is: 0.82\n",
      "In iteratin 107 mean ep length is: 2.19 and mean ep rewards is: 0.89\n",
      "In iteratin 108 mean ep length is: 2.23 and mean ep rewards is: 0.93\n",
      "In iteratin 109 mean ep length is: 1.97 and mean ep rewards is: 0.73\n",
      "In iteratin 110 mean ep length is: 1.88 and mean ep rewards is: 0.67\n",
      "In iteratin 111 mean ep length is: 1.90 and mean ep rewards is: 0.70\n",
      "In iteratin 112 mean ep length is: 2.00 and mean ep rewards is: 0.77\n",
      "In iteratin 113 mean ep length is: 1.89 and mean ep rewards is: 0.68\n",
      "In iteratin 114 mean ep length is: 1.79 and mean ep rewards is: 0.61\n",
      "In iteratin 115 mean ep length is: 1.82 and mean ep rewards is: 0.63\n",
      "In iteratin 116 mean ep length is: 1.75 and mean ep rewards is: 0.57\n",
      "In iteratin 117 mean ep length is: 1.69 and mean ep rewards is: 0.54\n",
      "In iteratin 118 mean ep length is: 1.77 and mean ep rewards is: 0.59\n",
      "In iteratin 119 mean ep length is: 1.89 and mean ep rewards is: 0.68\n",
      "In iteratin 120 mean ep length is: 1.83 and mean ep rewards is: 0.64\n",
      "In iteratin 121 mean ep length is: 1.79 and mean ep rewards is: 0.61\n",
      "In iteratin 122 mean ep length is: 1.95 and mean ep rewards is: 0.73\n",
      "In iteratin 123 mean ep length is: 2.03 and mean ep rewards is: 0.78\n",
      "In iteratin 124 mean ep length is: 1.99 and mean ep rewards is: 0.75\n",
      "In iteratin 125 mean ep length is: 2.31 and mean ep rewards is: 0.97\n",
      "In iteratin 126 mean ep length is: 2.35 and mean ep rewards is: 0.98\n",
      "In iteratin 127 mean ep length is: 2.20 and mean ep rewards is: 0.89\n",
      "In iteratin 128 mean ep length is: 2.42 and mean ep rewards is: 1.03\n",
      "In iteratin 129 mean ep length is: 2.24 and mean ep rewards is: 0.92\n",
      "In iteratin 130 mean ep length is: 2.45 and mean ep rewards is: 1.05\n",
      "In iteratin 131 mean ep length is: 2.53 and mean ep rewards is: 1.13\n",
      "In iteratin 132 mean ep length is: 2.34 and mean ep rewards is: 0.97\n",
      "In iteratin 133 mean ep length is: 2.33 and mean ep rewards is: 0.98\n",
      "In iteratin 134 mean ep length is: 2.54 and mean ep rewards is: 1.08\n",
      "In iteratin 135 mean ep length is: 2.73 and mean ep rewards is: 1.25\n",
      "In iteratin 136 mean ep length is: 2.86 and mean ep rewards is: 1.32\n",
      "In iteratin 137 mean ep length is: 2.96 and mean ep rewards is: 1.41\n",
      "In iteratin 138 mean ep length is: 2.59 and mean ep rewards is: 1.16\n",
      "In iteratin 139 mean ep length is: 2.68 and mean ep rewards is: 1.20\n",
      "In iteratin 140 mean ep length is: 2.82 and mean ep rewards is: 1.31\n",
      "In iteratin 141 mean ep length is: 2.70 and mean ep rewards is: 1.23\n",
      "In iteratin 142 mean ep length is: 2.63 and mean ep rewards is: 1.18\n",
      "In iteratin 143 mean ep length is: 2.71 and mean ep rewards is: 1.24\n",
      "In iteratin 144 mean ep length is: 2.56 and mean ep rewards is: 1.14\n",
      "In iteratin 145 mean ep length is: 2.57 and mean ep rewards is: 1.18\n",
      "In iteratin 146 mean ep length is: 2.26 and mean ep rewards is: 0.95\n",
      "In iteratin 147 mean ep length is: 2.42 and mean ep rewards is: 1.09\n",
      "In iteratin 148 mean ep length is: 2.21 and mean ep rewards is: 0.93\n",
      "In iteratin 149 mean ep length is: 2.38 and mean ep rewards is: 1.02\n",
      "In iteratin 150 mean ep length is: 2.11 and mean ep rewards is: 0.84\n",
      "In iteratin 151 mean ep length is: 2.26 and mean ep rewards is: 0.96\n",
      "In iteratin 152 mean ep length is: 2.20 and mean ep rewards is: 0.90\n",
      "In iteratin 153 mean ep length is: 2.06 and mean ep rewards is: 0.80\n",
      "In iteratin 154 mean ep length is: 2.34 and mean ep rewards is: 0.97\n",
      "In iteratin 155 mean ep length is: 2.21 and mean ep rewards is: 0.89\n",
      "In iteratin 156 mean ep length is: 2.11 and mean ep rewards is: 0.83\n",
      "In iteratin 157 mean ep length is: 2.20 and mean ep rewards is: 0.91\n",
      "In iteratin 158 mean ep length is: 2.08 and mean ep rewards is: 0.84\n",
      "In iteratin 159 mean ep length is: 2.18 and mean ep rewards is: 0.90\n",
      "In iteratin 160 mean ep length is: 2.12 and mean ep rewards is: 0.83\n",
      "In iteratin 161 mean ep length is: 2.05 and mean ep rewards is: 0.80\n",
      "In iteratin 162 mean ep length is: 2.25 and mean ep rewards is: 0.94\n",
      "In iteratin 163 mean ep length is: 2.38 and mean ep rewards is: 1.04\n",
      "In iteratin 164 mean ep length is: 2.20 and mean ep rewards is: 0.88\n",
      "In iteratin 165 mean ep length is: 2.62 and mean ep rewards is: 1.21\n",
      "In iteratin 166 mean ep length is: 2.32 and mean ep rewards is: 0.98\n",
      "In iteratin 167 mean ep length is: 2.52 and mean ep rewards is: 1.12\n",
      "In iteratin 168 mean ep length is: 2.14 and mean ep rewards is: 0.87\n",
      "In iteratin 169 mean ep length is: 1.98 and mean ep rewards is: 0.75\n",
      "In iteratin 170 mean ep length is: 2.12 and mean ep rewards is: 0.84\n",
      "In iteratin 171 mean ep length is: 2.09 and mean ep rewards is: 0.83\n",
      "In iteratin 172 mean ep length is: 2.34 and mean ep rewards is: 1.00\n",
      "In iteratin 173 mean ep length is: 2.15 and mean ep rewards is: 0.88\n",
      "In iteratin 174 mean ep length is: 2.00 and mean ep rewards is: 0.78\n",
      "In iteratin 175 mean ep length is: 1.92 and mean ep rewards is: 0.72\n",
      "In iteratin 176 mean ep length is: 1.84 and mean ep rewards is: 0.66\n",
      "In iteratin 177 mean ep length is: 1.87 and mean ep rewards is: 0.68\n",
      "In iteratin 178 mean ep length is: 1.81 and mean ep rewards is: 0.62\n",
      "In iteratin 179 mean ep length is: 1.92 and mean ep rewards is: 0.71\n",
      "In iteratin 180 mean ep length is: 1.79 and mean ep rewards is: 0.61\n",
      "In iteratin 181 mean ep length is: 1.69 and mean ep rewards is: 0.54\n",
      "In iteratin 182 mean ep length is: 1.88 and mean ep rewards is: 0.67\n",
      "In iteratin 183 mean ep length is: 1.70 and mean ep rewards is: 0.56\n",
      "In iteratin 184 mean ep length is: 1.89 and mean ep rewards is: 0.69\n",
      "In iteratin 185 mean ep length is: 1.93 and mean ep rewards is: 0.71\n",
      "In iteratin 186 mean ep length is: 2.01 and mean ep rewards is: 0.77\n",
      "In iteratin 187 mean ep length is: 2.16 and mean ep rewards is: 0.87\n",
      "In iteratin 188 mean ep length is: 2.18 and mean ep rewards is: 0.88\n",
      "In iteratin 189 mean ep length is: 2.55 and mean ep rewards is: 1.11\n",
      "In iteratin 190 mean ep length is: 2.52 and mean ep rewards is: 1.06\n",
      "In iteratin 191 mean ep length is: 2.71 and mean ep rewards is: 1.19\n",
      "In iteratin 192 mean ep length is: 2.52 and mean ep rewards is: 1.03\n",
      "In iteratin 193 mean ep length is: 2.62 and mean ep rewards is: 1.11\n",
      "In iteratin 194 mean ep length is: 2.48 and mean ep rewards is: 1.01\n",
      "In iteratin 195 mean ep length is: 2.44 and mean ep rewards is: 1.01\n",
      "In iteratin 196 mean ep length is: 2.39 and mean ep rewards is: 0.98\n",
      "In iteratin 197 mean ep length is: 2.59 and mean ep rewards is: 1.15\n",
      "In iteratin 198 mean ep length is: 2.32 and mean ep rewards is: 0.96\n",
      "In iteratin 199 mean ep length is: 2.38 and mean ep rewards is: 1.01\n",
      "In iteratin 200 mean ep length is: 2.40 and mean ep rewards is: 1.04\n",
      "In iteratin 201 mean ep length is: 2.14 and mean ep rewards is: 0.87\n",
      "In iteratin 202 mean ep length is: 2.04 and mean ep rewards is: 0.78\n",
      "In iteratin 203 mean ep length is: 2.06 and mean ep rewards is: 0.80\n",
      "In iteratin 204 mean ep length is: 2.08 and mean ep rewards is: 0.82\n",
      "In iteratin 205 mean ep length is: 2.21 and mean ep rewards is: 0.92\n",
      "In iteratin 206 mean ep length is: 2.16 and mean ep rewards is: 0.87\n",
      "In iteratin 207 mean ep length is: 2.19 and mean ep rewards is: 0.91\n",
      "In iteratin 208 mean ep length is: 2.21 and mean ep rewards is: 0.94\n",
      "In iteratin 209 mean ep length is: 2.31 and mean ep rewards is: 1.00\n",
      "In iteratin 210 mean ep length is: 2.54 and mean ep rewards is: 1.13\n",
      "In iteratin 211 mean ep length is: 2.39 and mean ep rewards is: 1.00\n",
      "In iteratin 212 mean ep length is: 2.28 and mean ep rewards is: 0.90\n",
      "In iteratin 213 mean ep length is: 2.23 and mean ep rewards is: 0.86\n",
      "In iteratin 214 mean ep length is: 2.14 and mean ep rewards is: 0.77\n",
      "In iteratin 215 mean ep length is: 2.25 and mean ep rewards is: 0.84\n",
      "In iteratin 216 mean ep length is: 2.38 and mean ep rewards is: 0.97\n",
      "In iteratin 217 mean ep length is: 2.39 and mean ep rewards is: 0.96\n",
      "In iteratin 218 mean ep length is: 2.40 and mean ep rewards is: 0.99\n",
      "In iteratin 219 mean ep length is: 2.48 and mean ep rewards is: 1.04\n",
      "In iteratin 220 mean ep length is: 2.39 and mean ep rewards is: 0.98\n",
      "In iteratin 221 mean ep length is: 2.44 and mean ep rewards is: 1.03\n",
      "In iteratin 222 mean ep length is: 2.32 and mean ep rewards is: 0.96\n",
      "In iteratin 223 mean ep length is: 2.21 and mean ep rewards is: 0.88\n",
      "In iteratin 224 mean ep length is: 2.19 and mean ep rewards is: 0.88\n",
      "In iteratin 225 mean ep length is: 2.70 and mean ep rewards is: 1.25\n",
      "In iteratin 226 mean ep length is: 2.38 and mean ep rewards is: 0.98\n",
      "In iteratin 227 mean ep length is: 2.38 and mean ep rewards is: 1.03\n",
      "In iteratin 228 mean ep length is: 2.63 and mean ep rewards is: 1.25\n",
      "In iteratin 229 mean ep length is: 2.43 and mean ep rewards is: 1.12\n",
      "In iteratin 230 mean ep length is: 2.23 and mean ep rewards is: 0.96\n",
      "In iteratin 231 mean ep length is: 2.20 and mean ep rewards is: 0.93\n",
      "In iteratin 232 mean ep length is: 2.17 and mean ep rewards is: 0.89\n",
      "In iteratin 233 mean ep length is: 2.49 and mean ep rewards is: 1.12\n",
      "In iteratin 234 mean ep length is: 2.58 and mean ep rewards is: 1.15\n",
      "In iteratin 235 mean ep length is: 2.75 and mean ep rewards is: 1.29\n",
      "In iteratin 236 mean ep length is: 2.56 and mean ep rewards is: 1.12\n",
      "In iteratin 237 mean ep length is: 2.73 and mean ep rewards is: 1.25\n",
      "In iteratin 238 mean ep length is: 2.62 and mean ep rewards is: 1.18\n",
      "In iteratin 239 mean ep length is: 2.55 and mean ep rewards is: 1.13\n",
      "In iteratin 240 mean ep length is: 2.58 and mean ep rewards is: 1.16\n",
      "In iteratin 241 mean ep length is: 2.54 and mean ep rewards is: 1.11\n",
      "In iteratin 242 mean ep length is: 2.39 and mean ep rewards is: 0.99\n",
      "In iteratin 243 mean ep length is: 2.51 and mean ep rewards is: 1.07\n",
      "In iteratin 244 mean ep length is: 2.51 and mean ep rewards is: 1.06\n",
      "In iteratin 245 mean ep length is: 2.15 and mean ep rewards is: 0.81\n",
      "In iteratin 246 mean ep length is: 2.34 and mean ep rewards is: 0.94\n",
      "In iteratin 247 mean ep length is: 2.24 and mean ep rewards is: 0.90\n",
      "In iteratin 248 mean ep length is: 2.12 and mean ep rewards is: 0.78\n",
      "In iteratin 249 mean ep length is: 2.15 and mean ep rewards is: 0.81\n",
      "In iteratin 250 mean ep length is: 2.39 and mean ep rewards is: 0.98\n",
      "In iteratin 251 mean ep length is: 2.26 and mean ep rewards is: 0.90\n",
      "In iteratin 252 mean ep length is: 2.54 and mean ep rewards is: 1.09\n",
      "In iteratin 253 mean ep length is: 2.62 and mean ep rewards is: 1.12\n",
      "In iteratin 254 mean ep length is: 2.38 and mean ep rewards is: 0.98\n",
      "In iteratin 255 mean ep length is: 2.15 and mean ep rewards is: 0.84\n",
      "In iteratin 256 mean ep length is: 2.32 and mean ep rewards is: 0.94\n",
      "In iteratin 257 mean ep length is: 2.49 and mean ep rewards is: 1.12\n",
      "In iteratin 258 mean ep length is: 2.41 and mean ep rewards is: 1.03\n",
      "In iteratin 259 mean ep length is: 2.27 and mean ep rewards is: 0.95\n",
      "In iteratin 260 mean ep length is: 2.15 and mean ep rewards is: 0.87\n",
      "In iteratin 261 mean ep length is: 2.03 and mean ep rewards is: 0.79\n",
      "In iteratin 262 mean ep length is: 2.11 and mean ep rewards is: 0.86\n",
      "In iteratin 263 mean ep length is: 1.99 and mean ep rewards is: 0.75\n",
      "In iteratin 264 mean ep length is: 2.07 and mean ep rewards is: 0.80\n",
      "In iteratin 265 mean ep length is: 2.11 and mean ep rewards is: 0.82\n",
      "In iteratin 266 mean ep length is: 2.46 and mean ep rewards is: 1.06\n",
      "In iteratin 267 mean ep length is: 2.29 and mean ep rewards is: 0.96\n",
      "In iteratin 268 mean ep length is: 2.29 and mean ep rewards is: 0.94\n",
      "In iteratin 269 mean ep length is: 2.09 and mean ep rewards is: 0.80\n",
      "In iteratin 270 mean ep length is: 2.24 and mean ep rewards is: 0.88\n",
      "In iteratin 271 mean ep length is: 2.24 and mean ep rewards is: 0.90\n",
      "In iteratin 272 mean ep length is: 2.39 and mean ep rewards is: 1.01\n",
      "In iteratin 273 mean ep length is: 2.21 and mean ep rewards is: 0.90\n",
      "In iteratin 274 mean ep length is: 2.28 and mean ep rewards is: 0.90\n",
      "In iteratin 275 mean ep length is: 2.42 and mean ep rewards is: 1.06\n",
      "In iteratin 276 mean ep length is: 2.44 and mean ep rewards is: 1.05\n",
      "In iteratin 277 mean ep length is: 2.42 and mean ep rewards is: 1.02\n",
      "In iteratin 278 mean ep length is: 2.44 and mean ep rewards is: 1.02\n",
      "In iteratin 279 mean ep length is: 2.33 and mean ep rewards is: 0.96\n",
      "In iteratin 280 mean ep length is: 2.49 and mean ep rewards is: 1.07\n",
      "In iteratin 281 mean ep length is: 2.37 and mean ep rewards is: 0.98\n",
      "In iteratin 282 mean ep length is: 2.28 and mean ep rewards is: 0.92\n"
     ]
    }
   ],
   "source": [
    "t_so_far = 0 # Timesteps simulated so far\n",
    "i_so_far = 0 # Iterations ran so far\n",
    "last_rewards = deque(maxlen = 5)\n",
    "max_mean_reward = 0\n",
    "scores = []\n",
    "#with open(\"scores_rollpitch_2layer256_epoch5_lr50_baseobsAndSUMcontact\", \"rb\") as fp:  scores = pickle.load(fp)\n",
    "    \n",
    "while t_so_far < total_timesteps:                                                      \n",
    "    batch_obs, batch_acts, batch_log_probs, batch_rtgs, batch_lens, batch_rews, sum_ep_rew = \\\n",
    "    rollout(model, pid_time, kp, kd, ki, actor, cov_mat, gamma, rew_coef, distribution,\n",
    "            timesteps_per_batch, max_timesteps_per_episode, action_range,\n",
    "            device, 'train', ball, obs_dim)\n",
    "    \n",
    "    t_so_far += np.sum(batch_lens)\n",
    "    i_so_far += 1\n",
    "\n",
    "    V, _ = evaluate(actor, critic, cov_mat, batch_obs, batch_acts, distribution, action_range)\n",
    "    A_k = batch_rtgs - V.detach()                                                                 \n",
    "    A_k = (A_k - A_k.mean()) / (A_k.std() + 1e-10)\n",
    "\n",
    "    # This is the loop where we update our network for some n epochs\n",
    "    for epoch in range(Epochs):                                                       \n",
    "        # Calculate V_phi and pi_theta(a_t | s_t)\n",
    "        V, curr_log_probs = evaluate(actor, critic, cov_mat, batch_obs, batch_acts,\n",
    "                                     distribution, action_range)\n",
    "        ratios = torch.exp(curr_log_probs - batch_log_probs)\n",
    "\n",
    "        # Calculate surrogate losses.\n",
    "        surr1 = ratios * A_k\n",
    "        surr2 = torch.clamp(ratios, 1 - clip, 1 + clip) * A_k\n",
    " \n",
    "        entropy = -(batch_acts * batch_log_probs.reshape(batch_acts.shape[0],-1)\n",
    "                   ).sum(dim=1).mean()\n",
    "        loss_e = -entropy_beta * entropy\n",
    "        \n",
    "        actor_loss = (-torch.min(surr1, surr2)).mean().to(device)# + loss_e\n",
    "        critic_loss = nn.MSELoss()(V, batch_rtgs)# + loss_e\n",
    "\n",
    "        actor_optim.zero_grad()\n",
    "        actor_loss.backward(retain_graph=True)\n",
    "        actor_optim.step()\n",
    "\n",
    "        critic_optim.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        critic_optim.step()\n",
    "    \n",
    "    last_rewards.append(np.mean(sum_ep_rew))\n",
    "    scores.append(np.mean(sum_ep_rew))\n",
    "\n",
    "    print(\"In iteratin\", i_so_far, \"mean ep length is:\", \"{:.2f}\".format(np.mean(batch_lens)),\n",
    "          'and mean ep rewards is:', \"{:.2f}\".format(np.mean(sum_ep_rew)))\n",
    "    if np.mean(sum_ep_rew) > max_mean_reward:\n",
    "        os.system('play -nq -t alsa synth {} sine {}'.format(0.25, 440))\n",
    "        print('maximum mean reward is acheived!')\n",
    "        torch.save(actor.state_dict(), 'act_rollpitch_2layer256_epoch5_lr50_baseobsAndSUMcontact')\n",
    "        torch.save(critic.state_dict(), 'crt_rollpitch_2layer256_epoch5_lr50_baseobsAndSUMcontact')\n",
    "        with open(\"scores_rollpitch_2layer256_epoch5_lr50_baseobsAndSUMcontact\", \"wb\") as fp: pickle.dump(scores, fp)\n",
    "        max_mean_reward = np.mean(sum_ep_rew)\n",
    "        \n",
    "    if np.mean(last_rewards) > 0.92*max_timesteps_per_episode or np.mean(sum_ep_rew)<0.0001:\n",
    "        torch.save(actor.state_dict(), 'act_rollpitch_2layer256_epoch5_lr50_baseobsAndSUMcontact')\n",
    "        torch.save(critic.state_dict(), 'crt_rollpitch_2layer256_epoch5_lr50_baseobsAndSUMcontact')       \n",
    "        os.system('play -nq -t alsa synth {} sine {}'.format(3, 440))\n",
    "        print(\"solved!\")\n",
    "        with open(\"scores_rollpitch_2layer256_epoch5_lr50_baseobsAndSUMcontact\", \"wb\") as fp: pickle.dump(scores, fp)\n",
    "        break\n",
    "\n",
    "reg = LinearRegression().fit(np.arange(len(scores)).reshape(-1, 1),\n",
    "                             np.array(scores).reshape(-1, 1))\n",
    "score_pred = reg.predict(np.arange(len(scores)).reshape(-1, 1))\n",
    "plt.plot(scores)\n",
    "plt.ylabel('rewards')\n",
    "plt.xlabel('iterations')\n",
    "plt.plot(score_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c11439-4a3d-4105-9d89-ed7d89d56908",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(actor.state_dict(), 'act_rollpitch_2layer256_epoch5_lr50_baseobsAndcontact')\n",
    "torch.save(critic.state_dict(), 'crt_rollpitch_2layer256_epoch5_lr50_baseobsAndcontact')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57c8169-a823-464d-b286-2a6a386a8ec7",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"scores_rollpitch_2layer256_epoch5_contfeet2\", \"rb\") as fp:  scores = pickle.load(fp)\n",
    "reg = LinearRegression().fit(np.arange(len(scores)).reshape(-1, 1),\n",
    "                             np.array(scores).reshape(-1, 1))\n",
    "score_pred = reg.predict(np.arange(len(scores)).reshape(-1, 1))\n",
    "\n",
    "plt.plot(scores)\n",
    "plt.ylabel('mean rewards')\n",
    "plt.xlabel('iterations')\n",
    "plt.plot(score_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44aa4b44-4cb7-4b43-b45c-290b5b3e1316",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"scores_rollpitch_2layer256_epoch5\", \"rb\") as fp:  scores = pickle.load(fp)\n",
    "reg = LinearRegression().fit(np.arange(len(scores)).reshape(-1, 1),\n",
    "                             np.array(scores).reshape(-1, 1))\n",
    "score_pred = reg.predict(np.arange(len(scores)).reshape(-1, 1))\n",
    "\n",
    "plt.plot(scores)\n",
    "plt.ylabel('mean rewards')\n",
    "plt.xlabel('iterations')\n",
    "plt.plot(score_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0267ff5f-a68c-4f29-8c37-1742f9463393",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(10):\n",
    "    sim = MjSim(model)\n",
    "    sim = initial_condition(sim, model, kp, kd, ki, state_range, 'test', ball)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df66460-4848-4054-8a44-13483691c92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mj_path = mujoco_py.utils.discover_mujoco()\n",
    "xml_path = os.path.join(mj_path, 'model', 'aiutman_lower_rollpitch.xml')\n",
    "model = load_model_from_path(xml_path)\n",
    "sim = MjSim(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2ab1fc-3b56-46ec-8dfe-581052d6b97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "viewer = MjViewer(sim)\n",
    "c_array = np.zeros(6, dtype=np.float64)\n",
    "for ii in range(200):\n",
    "    sim.data.xfrc_applied[1][1]  = 10\n",
    "    sim.step()\n",
    "    #print('\\n number of contacts', sim.data.ncon)\n",
    "    for i in range(sim.data.ncon):\n",
    "        #print(i)\n",
    "        contact = sim.data.contact[i]\n",
    "        #geom1_body = sim.model.geom_bodyid[contact.geom1]\n",
    "        #geom2_body = sim.model.geom_bodyid[contact.geom2]\n",
    "        #print('geom1', contact.geom1, sim.model.geom_id2name(contact.geom1),'body', geom1_body)\n",
    "        #print('geom2', contact.geom2, sim.model.geom_id2name(contact.geom2),'body', geom2_body)\n",
    "        print()\n",
    "        if contact.geom2 in [10, 19]:\n",
    "            print(i)\n",
    "            #print('geom1', contact.geom1)\n",
    "            print('geom2', contact.geom2)\n",
    "            # Use internal functions to read out mj_contactForce\n",
    "            mujoco_py.functions.mj_contactForce(sim.model, sim.data, i, c_array)\n",
    "            print('c_array', c_array[0])\n",
    "            print(sim.data.sensordata[8], sim.data.sensordata[11],\n",
    "                  sim.data.sensordata[14], sim.data.sensordata[17])\n",
    "            print(sim.data.sensordata[20], sim.data.sensordata[23],\n",
    "                  sim.data.sensordata[26], sim.data.sensordata[29])\n",
    "            \n",
    "    contact_forces = contact_force(sim)\n",
    "    print(contact_forces)\n",
    "    viewer.render()\n",
    "    time.sleep(0.01)\n",
    "glfw.destroy_window(viewer.window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf83f502-1360-4f86-b84c-e8a226ab16f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
